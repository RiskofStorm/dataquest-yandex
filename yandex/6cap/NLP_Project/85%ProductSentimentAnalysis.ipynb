{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# uncomment and run it first!\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load nlp_process_data.py\n",
    "\n",
    "def process_data(data, func):\n",
    "    pr_data = data.copy()\n",
    "    index = data.shape[0]\n",
    "    for i in range(index):\n",
    "        pr_data[i] = \" \".join([func(w) for w in data[i].split()])\n",
    "    return pr_data\n",
    "\n",
    "\n",
    "def build_pipe(vect, model, stopwords=None, ngram_range=(1,1), analyzer='word'):\n",
    "    return Pipeline([(\"count\", vect(stop_words=stopwords, ngram_range=ngram_range, analyzer=analyzer)),\n",
    "                     (\"model\", model())])\n",
    "\n",
    "\n",
    "def train_test_models(X_train, y_train, models_cls=None, vectorizer_cls=None, \n",
    "                      random_state=None, min_df=None,stopwords=None,\n",
    "                      ngram_range=(1,1), analyzer='word',\n",
    "                      vectorizer_names=None, model_names=None):\n",
    "    results = list()\n",
    "    mean = list()\n",
    "    if not models_cls or not vectorizer_names:\n",
    "        models_cls = [LogisticRegression,\n",
    "                      RandomForestClassifier,\n",
    "                      LinearSVC,\n",
    "                      SGDClassifier]\n",
    "        model_names = [\"LogReg\", \"RF Clas.\", \"LinearSVC\", \"SGD Clas.\"]\n",
    "    if not vectorizer_cls or not model_names:\n",
    "        vectorizer_cls = [TfidfVectorizer, CountVectorizer]\n",
    "        vectorizer_names = [\"TfidfVec\", \"CntVec\"]\n",
    "       \n",
    "    _vectorizer_names = iter(vectorizer_names)\n",
    "    for vectorizer in vectorizer_cls:\n",
    "        vector_name = _vectorizer_names.__next__()\n",
    "        _model_names = iter(model_names)\n",
    "        for model in models_cls:\n",
    "            pipe = build_pipe(vectorizer, model, stopwords=stopwords,\n",
    "                              ngram_range=ngram_range, analyzer=analyzer)\n",
    "            score = cross_val_score(pipe, X_train, y_train, scoring='accuracy')\n",
    "            mean.append(np.mean(score))\n",
    "            results.append(\"model: {}, vectorizer: {}  scores: {}, mean: {}\".format(_model_names.__next__(),\n",
    "                                                                                       vector_name, score,\n",
    "                                                                                       round(np.mean(score),4)))\n",
    "            \n",
    "    return results, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/winter/vm/github/kaggle/Product_Sentiment/\"\n",
    "data = open(path+\"test.csv\", \"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \" \".join(data).split(\"</review>\")[:-1]\n",
    "X_ = [text.replace('<review>', \"\").replace(\"\\n\", \"\") for text in data]\n",
    "stop_words = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting parse_yandex_market.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile parse_yandex_market.py\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import bs4\n",
    "\n",
    "def parse_page(url):\n",
    "    time.sleep(random.randint(5,30))\n",
    "    \n",
    "    text = requests.get(url)\n",
    "    text.encoding = \"utf-8\"\n",
    "    \n",
    "    parser = bs4.BeautifulSoup(text.text, 'lxml')\n",
    "    data = parser.findAll('div', attrs={\"class\":\"n-snippet-card-review__right\"})\n",
    "    \n",
    "    full_revs = [\" \".join([t.text for t in i.find_all('dd')]) for i in data]\n",
    "\n",
    "    rating_revs = [1 if float(i.text) > 3 else 0  for i in \n",
    "                   parser.findAll('div', attrs={'class':'rating__value'})]\n",
    "    \n",
    "    return list(zip(full_revs, rating_revs))\n",
    "\n",
    "url = \"\"\"https://market.yandex.ru/catalog--mobilnye-telefony-otzyvy-pokupatelei/54726/\n",
    "list?show-reviews=1&page={}&local-offers-first=0&onstock=0&how=quality\"\"\"\n",
    "\n",
    "pool = Pool(3)\n",
    "url_list = [url.format(i) for i in range(1,51)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    map_results = pool.map(parse_page, url_list)\n",
    "    with open('parsing_revievs_yandexMarket.txt','w+') as out_f:\n",
    "        out_f.write(\"\\n\".join(map_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
