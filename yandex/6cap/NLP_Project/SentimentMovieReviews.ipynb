{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('movie_reviews')\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "def write_ans(file, numb):\n",
    "    with open(\"answer \" + str(numb) + \".txt\", \"w+\") as f:\n",
    "        f.write(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_id = movie_reviews.fileids('neg')\n",
    "pos_id = movie_reviews.fileids('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_rev = [\" \".join(list(movie_reviews.words(fileids=[f]))) for f in neg_id]\n",
    "pos_rev = [\" \".join(list(movie_reviews.words(fileids=[f]))) for f in pos_id]\n",
    "sum_rev = neg_rev + pos_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of neg and pos reviews in movie reviews corpus:\n",
      "neg 1000\n",
      "pos 1000\n",
      "all 2000\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of neg and pos reviews in movie reviews corpus:\")\n",
    "print(\"neg\",len(neg_rev))\n",
    "print(\"pos\",len(pos_rev))\n",
    "print(\"all\", 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CountVectorizer()\n",
    "X_train = c.fit_transform(sum_rev)\n",
    "y_train = [0 for i in range(len(neg_rev))] + [1 for i in  range(len(pos_rev))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features(words) 39659\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features(words)\", len(c.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=42)\n",
    "count_vec = CountVectorizer()\n",
    "log_countvec = Pipeline([(\"count\", count_vec),(\"log\",model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = cross_val_score(log_countvec, sum_rev, y_train, scoring='accuracy')\n",
    "roc_auc_score = cross_val_score(log_countvec, sum_rev, y_train, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8360216503929078"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9107825058014015"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = LogisticRegression(random_state=1)\n",
    "coef = model_.fit(X_train, y_train).coef_.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad unfortunately'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_valuable_features = [coef.index(i) for i in sorted(coef)[:2]]\n",
    "features = np.array(c.get_feature_names())\n",
    "answer = \" \".join(features[most_valuable_features].tolist())\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_ans(str(len(sum_rew)),\" rew_full\")\n",
    "# write_ans(str(len(pos_rew)/len(sum_rew)), \" pos_coef\")\n",
    "# write_ans(str(len(c.get_feature_names())), \" numb_features\")\n",
    "# write_ans(str(np.mean(acc_score)), \" acc_score\")\n",
    "# write_ans(str(np.mean(roc_auc_score)), \" roc_auc_score\")\n",
    "# write_ans(answer, \" most_val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean,std accuracy_score of CountVectorizer_LogReg 5 fold cross val:  0.8415000000000001 0.01677796173556255\n",
      "mean,std accuracy_score of TfidfVectorizer_LogReg 5 fold cross val:  0.8415000000000001 0.01677796173556255\n"
     ]
    }
   ],
   "source": [
    "cv = 5\n",
    "\n",
    "acc_score = cross_val_score(log_countvec, sum_rev, y_train, scoring='accuracy', cv=cv)\n",
    "\n",
    "a_mean = np.mean(acc_score)\n",
    "a_std = np.std(acc_score)\n",
    "print(\"mean,std accuracy_score of CountVectorizer_LogReg 5 fold cross val: \",a_mean, a_std)\n",
    "\n",
    "\n",
    "log_tfidf = Pipeline([(\"count\", TfidfVectorizer()), (\"log\", model)])\n",
    "cv_logreg = cross_val_score(log_countvec, sum_rev, y_train, scoring=\"accuracy\", cv=cv)\n",
    "\n",
    "b_mean = np.mean(cv_logreg)\n",
    "b_std = np.std(cv_logreg)\n",
    "print(\"mean,std accuracy_score of TfidfVectorizer_LogReg 5 fold cross val: \",b_mean, b_std)\n",
    "\n",
    "# write_ans(\" \".join([str(i) for i in [a_mean, a_std, b_mean, b_std]]), \"_ab_mean_std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.82  , 0.85  , 0.8325, 0.8525, 0.84  ]), array([0.7925, 0.825 , 0.8025, 0.8175, 0.8275])]\n"
     ]
    }
   ],
   "source": [
    "# Try to use min_df in CountVectorizer\n",
    "min_df = [10,50]\n",
    "results = list()\n",
    "\n",
    "for min_df_ in min_df:\n",
    "    pipe = Pipeline([(\"count\", CountVectorizer(min_df=min_df_)), (\"log\", model)])\n",
    "    results.append(cross_val_score(pipe, sum_rev, y_train, scoring=\"accuracy\", cv=cv))\n",
    "    \n",
    "# write_ans(str(np.mean(results[0])) + \" \" + str(np.mean(results[1])), \"min_df\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding worst model\n",
    "models = [LogisticRegression(random_state=42), LinearSVC(random_state=42), \n",
    "          SGDClassifier(random_state=42)]\n",
    "\n",
    "model_results = list()\n",
    "for mod in models:\n",
    "    pipe = Pipeline([(\"count\", CountVectorizer()), (\"mod\", mod)])\n",
    "    model_results.append(cross_val_score(pipe, sum_rev, y_train, scoring=\"accuracy\", cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8415000000000001, 0.8325000000000001, 0.74]\n"
     ]
    }
   ],
   "source": [
    "model_results = [np.mean(i) for i in model_results]\n",
    "print(model_results)\n",
    "# write_ans(str(min(model_results)), \"worst_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.841, 0.8390000000000001]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same models w/o stopwords in dictionary\n",
    "\n",
    "\n",
    "mod = [stopwords.words(\"english\"), \"english\"]\n",
    "\n",
    "model_results_stopwords = list()\n",
    "for sw in mod:\n",
    "    pipe = Pipeline([(\"count\", CountVectorizer(stop_words=sw)), (\"mod\", LogisticRegression())])\n",
    "    model_results_stopwords.append(cross_val_score(pipe, sum_rev, y_train, scoring=\"accuracy\", cv=cv))\n",
    "model_results_stopwords = [np.mean(i) for i in model_results_stopwords]\n",
    "model_results_stopwords    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of the models passed barrier of .80;   \n",
    "What's a good and predictible result - stopwords stoils dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added bigrams to CountVectorizer\n",
    "\n",
    "model_results_bigram = list()\n",
    "model_results_bigram.append(cross_val_score(Pipeline([(\"count\", CountVectorizer(stop_words=mod[0], ngram_range=(1,2))), (\"mod\", LogisticRegression(random_state=42))]), sum_rev, y_train, scoring=\"accuracy\", cv=cv))\n",
    "model_results_bigram.append(cross_val_score(Pipeline([(\"count\", CountVectorizer(stop_words=mod[0], ngram_range=(3,5), analyzer='char_wb')), (\"mod\", LogisticRegression(random_state=42))]), sum_rev, y_train, scoring=\"accuracy\", cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.8225, 0.8375, 0.8325, 0.865 , 0.86  ]),\n",
       " array([0.83  , 0.835 , 0.8125, 0.8075, 0.815 ])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_ans(str(np.mean(model_results_bigram[0])) + \" \" + str(np.mean(model_results_bigram[1])), \"ngramm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
