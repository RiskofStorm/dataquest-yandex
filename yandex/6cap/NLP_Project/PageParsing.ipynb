{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример парсинга страницы сайта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы получить html-код страницы нам потребуется библиотека requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "req = requests.get('http://zadolba.li/20160417')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "print req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n"
     ]
    }
   ],
   "source": [
    "print type(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print req.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам нужно как-то обрабатывать этот html-код. Для этого подойдет библиотека Beautiful Soup 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У bs4 весьма несложный интерфейс, хотя обращаться к документации на первых порах все же придется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser = bs4.BeautifulSoup(req.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print type(parser)\n",
    "# print parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим первый тег div, атрибут class у которого имеет значение 'text':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print parser.find('div', attrs={'class':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "x = parser.find('div', attrs={'class':'text'})\n",
    "print type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print x.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим тексты всех историй со страницы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n"
     ]
    }
   ],
   "source": [
    "y = parser.findAll('div', attrs={'class':'text'})\n",
    "print type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for result in y:\n",
    "#     print result.text\n",
    "#     print \"\\n------\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже рассмотренных простых действий достаточно для того, чтобы кое-как парсить сайт с известной вам структурой. Но если вы попробуете таким образом распарсить более одной страницы, скорее всего заметите, что это происходит очень медленно. Можно существенно ускориться, воспользовавшись библиотекой multiprocessing, чтобы параллельно парсить несколько страниц. Ниже приводится пример такого кода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile parse_zadolbali.py\n",
    "import requests\n",
    "import bs4\n",
    "from multiprocessing import Pool\n",
    "import codecs\n",
    "\n",
    "def parse_page(url):\n",
    "    text = requests.get(url).text\n",
    "    parser = bs4.BeautifulSoup(text, 'lxml')\n",
    "    x = parser.findAll('div', attrs={'class':'text'})\n",
    "    return [res.text for res in x]\n",
    "\n",
    "p = Pool(10)\n",
    "url_list = ['http://zadolba.li/201604' + '0' * int(n < 10) + str(n) for n in range(1, 18)]\n",
    "    \n",
    "if __name__ == '__main__':    \n",
    "    map_results = p.map(parse_page, url_list)\n",
    "    reduce_results = reduce(lambda x,y: x + y, map_results)\n",
    "    with codecs.open('parsing_results.txt', 'w', 'utf-8') as output_file:\n",
    "        print >> output_file, u'\\n'.join(reduce_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parse_zadolbali.py надо запускать из консоли, а не из блокнота - ipython notebook не слишком дружит с multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике прирост скорости парсинга наблюдается при увеличение пула примерно до 100, далее уже бессмысленно, но все-таки не стоит слишком усердствовать - 10-20 будет достаточно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Владельцы сайта, который вы парсите, могут не очень хотеть, чтобы вы это делали. Тем более, если вы делаете это очень активно. Возможно периодически вас будут на некоторое время банить. Вы можете написать парсер так, чтобы он проверял ответ на запрос и после нескольких неудачных попыток засыпал на сколько-то секунд. Можно пытаться делать вид, что ваш парсер вовсе не парсер, делая запросы через случайные интервалы времени. Но беспокоиться о повторных запросах, скорости работы и других нюансах не обязательно самому. Существуют питоновские библиотеки, специально предназначенные для парсинга сайтов.\n",
    "\n",
    "Одна из таких библиотек - Scrapy. При выполнении задания на следующей неделе вы можете попробовать воспользоваться ей. Возможно у вас будут некоторые сложности при установке и в процессе привыкания к css selectors, но в будущем умение использовать scrapy или другую готовую библиотеку для парсинга сайтов почти наверняка оправдает себя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
