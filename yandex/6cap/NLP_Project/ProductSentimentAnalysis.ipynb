{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# uncomment and run it first!\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, func):\n",
    "    pr_data = data.copy()\n",
    "    index = data.shape[0]\n",
    "    for i in range(index):\n",
    "        pr_data[i] = \" \".join([func(w) for w in data[i].split()])\n",
    "    return pr_data\n",
    "\n",
    "\n",
    "def build_pipe(vect, model, stopwords=None, ngram_range=(1,1), analyzer='word'):\n",
    "    return Pipeline([(\"count\", vect(stop_words=stopwords, ngram_range=ngram_range, analyzer=analyzer)),\n",
    "                     (\"model\", model())])\n",
    "\n",
    "\n",
    "def train_test_models(X_train, y_train, models_cls=None, vectorizer_cls=None, \n",
    "                      random_state=None, min_df=None,stopwords=None,\n",
    "                      ngram_range=(1,1), analyzer='word',\n",
    "                      vectorizer_names=None, model_names=None):\n",
    "    results = list()\n",
    "    mean = list()\n",
    "    if not models_cls:\n",
    "        models_cls = [LogisticRegression,\n",
    "                      RandomForestClassifier,\n",
    "                      LinearSVC,\n",
    "                      SGDClassifier]\n",
    "        model_names = [\"LogReg\", \"RF Clas.\", \"LinearSVC\", \"SGD Clas.\"]\n",
    "    if not vectorizer_cls:\n",
    "        vectorizer_cls = [TfidfVectorizer, CountVectorizer]\n",
    "        vectorizer_names = [\"TfidfVec\", \"CntVec\"]\n",
    "       \n",
    "    _vectorizer_names = iter(vectorizer_names)\n",
    "    for vectorizer in vectorizer_cls:\n",
    "        vector_name = _vectorizer_names.__next__()\n",
    "        _model_names = iter(model_names)\n",
    "        for model in models_cls:\n",
    "            pipe = build_pipe(vectorizer, model, stopwords=stopwords,\n",
    "                              ngram_range=ngram_range, analyzer=analyzer)\n",
    "            score = cross_val_score(pipe, X_train, y_train, scoring='accuracy')\n",
    "            mean.append(np.mean(score))\n",
    "            results.append(\"model: {}, vectorizer: {}  scores: {}, mean: {}\".format(_model_names.__next__(),\n",
    "                                                                                       vector_name, score,\n",
    "                                                                                       round(np.mean(score),4)))\n",
    "            \n",
    "    return results, mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/winter/vm/github/kaggle/Product_Sentiment/\"\n",
    "train = pd.read_csv(path + \"products_sentiment_train.tsv\", sep=\"\\t\", names=['text','bin'])\n",
    "test = pd.read_csv(path + \"products_sentiment_test.tsv\", sep=\"\\t\",names=['text'])\n",
    "sample = pd.read_csv(path + \"products_sentiment_sample_submission.csv\", sep=\"\\t\")\n",
    "\n",
    "test.drop(test.index[[0]],axis=0, inplace=True)\n",
    "\n",
    "# Default words\n",
    "X_train = train['text']\n",
    "y_train = train['bin']\n",
    "kaggle_X_test = test[\"text\"]\n",
    "\n",
    "# prosessed words\n",
    "lemma_X_train = process_data(train['text'], WordNetLemmatizer().lemmatize)\n",
    "snow_X_train = process_data(train['text'], SnowballStemmer('english').stem)\n",
    "porter_X_train = process_data(train['text'], PorterStemmer().stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i downloaded a trial version of computer associate ez firewall and antivirus and fell in love with a computer security system all over again .\n",
      " \n",
      "i download a trial version of comput associ ez firewal and antivirus and fell in love with a comput secur system all over again .\n",
      " \n",
      "i download a trial version of comput associ ez firewal and antiviru and fell in love with a comput secur system all over again .\n"
     ]
    }
   ],
   "source": [
    "# Each stem/lemm has different word handling\n",
    "print(lemma_X_train[1], end='\\n \\n')\n",
    "print(snow_X_train[1], end='\\n \\n')\n",
    "print(porter_X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV+LogR [0.76611694 0.74512744 0.77777778]\n",
      "Tfidf+ RandForesCls [0.69715142 0.69265367 0.71171171]\n",
      "Tfidf+ LiearSVC [0.79610195 0.77061469 0.78378378]\n",
      "Tfidf+ SGDcls [0.76611694 0.75712144 0.75525526]\n"
     ]
    }
   ],
   "source": [
    "print(\"CV+LogR\", cross_val_score(build_pipe(CountVectorizer, LogisticRegression), \n",
    "                                X_train, y_train, scoring=\"accuracy\"))\n",
    "print(\"Tfidf+ RandForesCls\", cross_val_score(build_pipe(TfidfVectorizer, RandomForestClassifier),\n",
    "                                            X_train, y_train, scoring=\"accuracy\"))\n",
    "print(\"Tfidf+ LiearSVC\",cross_val_score(build_pipe(TfidfVectorizer, LinearSVC), \n",
    "                                        X_train, y_train, scoring=\"accuracy\"))\n",
    "print(\"Tfidf+ SGDcls\",cross_val_score(build_pipe(CountVectorizer, SGDClassifier),\n",
    "                                      X_train, y_train, scoring=\"accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_test_models(X_train, y_train, models_cls=None, vectorizer_names=None, \n",
    "                      random_state=None, min_df=None,stopwords=None,\n",
    "                      ngram_range=(1,1), analyzer='word')\n",
    "                      \n",
    "                      \n",
    "datasets = [lemma_X_train, snow_X_train, porter_X_train]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "lemma_test = train_test_models(lemma_X_train, y_train, min_df=5)\n",
    "snow_test = train_test_models(snow_X_train, y_train, min_df=5)\n",
    "porter_test = train_test_models(porter_X_train, y_train, min_df=5)\n",
    "default_test = train_test_models(X_train, y_train, min_df=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to take and look at all of these model setups with different NL processing.   \n",
    "I woudn't take RandomForest if I doesn't know that it can be heavely hyperparam setted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model: LogReg, vectorizer: TfidfVec  scores: '\n",
      " '[0.78710645 0.74062969 0.77027027], mean: '\n",
      " '0.766',\n",
      " 'model: RF Clas., vectorizer: TfidfVec  scores: '\n",
      " '[0.71814093 0.70614693 0.7042042 ], mean: '\n",
      " '0.7095',\n",
      " 'model: LinearSVC, vectorizer: TfidfVec  '\n",
      " 'scores: [0.7916042  0.74812594 0.77177177], '\n",
      " 'mean: 0.7705',\n",
      " 'model: SGD Clas., vectorizer: TfidfVec  '\n",
      " 'scores: [0.76611694 0.75262369 0.75975976], '\n",
      " 'mean: 0.7595',\n",
      " 'model: LogReg, vectorizer: CntVec  scores: '\n",
      " '[0.7916042  0.75112444 0.77327327], mean: '\n",
      " '0.772',\n",
      " 'model: RF Clas., vectorizer: CntVec  scores: '\n",
      " '[0.70314843 0.70614693 0.73123123], mean: '\n",
      " '0.7135',\n",
      " 'model: LinearSVC, vectorizer: CntVec  scores: '\n",
      " '[0.76911544 0.73613193 0.76126126], mean: '\n",
      " '0.7555',\n",
      " 'model: SGD Clas., vectorizer: CntVec  scores: '\n",
      " '[0.73613193 0.73763118 0.74174174], mean: '\n",
      " '0.7385']\n",
      "\n",
      " OVERALL MEAN 0.7481267812039926\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(lemma_test[0], width=50, indent=1)\n",
    "print(\"\\n overall mean {}\".format(np.mean(lemma_test[1])).upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model: LogReg, vectorizer: TfidfVec  scores: '\n",
      " '[0.7826087  0.74662669 0.76726727], mean: '\n",
      " '0.7655',\n",
      " 'model: RF Clas., vectorizer: TfidfVec  scores: '\n",
      " '[0.72113943 0.68515742 0.73273273], mean: '\n",
      " '0.713',\n",
      " 'model: LinearSVC, vectorizer: TfidfVec  '\n",
      " 'scores: [0.79310345 0.75712144 0.79129129], '\n",
      " 'mean: 0.7805',\n",
      " 'model: SGD Clas., vectorizer: TfidfVec  '\n",
      " 'scores: [0.76011994 0.75412294 0.75525526], '\n",
      " 'mean: 0.7565',\n",
      " 'model: LogReg, vectorizer: CntVec  scores: '\n",
      " '[0.7916042  0.74512744 0.78978979], mean: '\n",
      " '0.7755',\n",
      " 'model: RF Clas., vectorizer: CntVec  scores: '\n",
      " '[0.73763118 0.70014993 0.74024024], mean: '\n",
      " '0.726',\n",
      " 'model: LinearSVC, vectorizer: CntVec  scores: '\n",
      " '[0.78110945 0.74212894 0.75525526], mean: '\n",
      " '0.7595',\n",
      " 'model: SGD Clas., vectorizer: CntVec  scores: '\n",
      " '[0.7826087  0.72413793 0.75675676], mean: '\n",
      " '0.7545']\n",
      "\n",
      " OVERALL MEAN 0.7538785974880428\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(snow_test[0], width=50, indent=1)\n",
    "print(\"\\n overall mean {}\".format(np.mean(snow_test[1])).upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model: LogReg, vectorizer: TfidfVec  scores: '\n",
      " '[0.7826087  0.74962519 0.76726727], mean: '\n",
      " '0.7665',\n",
      " 'model: RF Clas., vectorizer: TfidfVec  scores: '\n",
      " '[0.74362819 0.71814093 0.71771772], mean: '\n",
      " '0.7265',\n",
      " 'model: LinearSVC, vectorizer: TfidfVec  '\n",
      " 'scores: [0.7946027  0.75112444 0.77927928], '\n",
      " 'mean: 0.775',\n",
      " 'model: SGD Clas., vectorizer: TfidfVec  '\n",
      " 'scores: [0.7886057  0.75712144 0.74624625], '\n",
      " 'mean: 0.764',\n",
      " 'model: LogReg, vectorizer: CntVec  scores: '\n",
      " '[0.79310345 0.73913043 0.78528529], mean: '\n",
      " '0.7725',\n",
      " 'model: RF Clas., vectorizer: CntVec  scores: '\n",
      " '[0.72713643 0.69265367 0.70870871], mean: '\n",
      " '0.7095',\n",
      " 'model: LinearSVC, vectorizer: CntVec  scores: '\n",
      " '[0.7856072  0.73613193 0.75225225], mean: '\n",
      " '0.758',\n",
      " 'model: SGD Clas., vectorizer: CntVec  scores: '\n",
      " '[0.77211394 0.73613193 0.75675676], mean: '\n",
      " '0.755']\n",
      "\n",
      " OVERALL MEAN 0.7533741575158367\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(porter_test[0], width=50, indent=1)\n",
    "print(\"\\n overall mean {}\".format(np.mean(porter_test[1])).upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model: LogReg, vectorizer: TfidfVec  scores: '\n",
      " '[0.76611694 0.73763118 0.76876877], mean: '\n",
      " '0.7575',\n",
      " 'model: RF Clas., vectorizer: TfidfVec  scores: '\n",
      " '[0.72113943 0.68665667 0.73123123], mean: '\n",
      " '0.713',\n",
      " 'model: LinearSVC, vectorizer: TfidfVec  '\n",
      " 'scores: [0.78410795 0.75262369 0.76876877], '\n",
      " 'mean: 0.7685',\n",
      " 'model: SGD Clas., vectorizer: TfidfVec  '\n",
      " 'scores: [0.77061469 0.74362819 0.75525526], '\n",
      " 'mean: 0.7565',\n",
      " 'model: LogReg, vectorizer: CntVec  scores: '\n",
      " '[0.7916042  0.74212894 0.78828829], mean: '\n",
      " '0.774',\n",
      " 'model: RF Clas., vectorizer: CntVec  scores: '\n",
      " '[0.70914543 0.70014993 0.72522523], mean: '\n",
      " '0.7115',\n",
      " 'model: LinearSVC, vectorizer: CntVec  scores: '\n",
      " '[0.76011994 0.72563718 0.76576577], mean: '\n",
      " '0.7505',\n",
      " 'model: SGD Clas., vectorizer: CntVec  scores: '\n",
      " '[0.74812594 0.70164918 0.71921922], mean: '\n",
      " '0.723']\n",
      "\n",
      " OVERALL MEAN 0.7443167492830162\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(default_test[0], width=50, indent=1)\n",
    "print(\"\\n overall mean {}\".format(np.mean(default_test[1])).upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm taking defaults, because hyperparameter optimization dosen't show improvements in a models.\n",
    "# It seems snowball stemmer does better result, still 0.01 is not much. Poor improvements from NLP\n",
    "# That means we can test another classificator models or imporove training dataset\n",
    "\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "log = LogisticRegression()\n",
    "cntvec = TfidfVectorizer(min_df=2, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score:  0.896\n",
      "vocab capacity: 5367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7530068812930081"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit_transform X_train X_test data, train Logistic Regression \n",
    "\n",
    "X_train_ = X_train.copy()\n",
    "y_train = y_train.copy()\n",
    "kaggle_X_test_ = kaggle_X_test.copy()\n",
    "\n",
    "cntvec.fit(X_train_)\n",
    "X_train_ = cntvec.transform(X_train_)\n",
    "\n",
    "log.fit(X_train_, y_train)\n",
    "log_pred = log.predict(X_train_)\n",
    "\n",
    "log_acc = accuracy_score(y_train, log_pred)\n",
    "\n",
    "print(\"model score: \",log_acc)\n",
    "print(\"vocab capacity:\",len(cntvec.vocabulary_))\n",
    "np.mean(cross_val_score(log, X_train_, y_train, scoring='accuracy', cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_test_pred = pd.DataFrame({'y':log.predict(cntvec.transform(kaggle_X_test_))})\n",
    "log_test_pred.index.name='id'\n",
    "log_test_pred.to_csv(\"submission.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(['not', 'only', 'would', 'after', 'norton', 'they', 'but',\n",
      "       'problem', 'does not', 'buttons', 'when', 'annoying', 'is not',\n",
      "       'unfortunately', 'useless', 'times', 'to', 'sometimes', 'them',\n",
      "       'their'], dtype='<U25')\n"
     ]
    }
   ],
   "source": [
    "# Everything what I've tried seems not working much with such eazy kaggle comp.\n",
    "# My thoughts - dataset is poor on expression words such as \"like\", \"hate\" and others bad/good words\n",
    "\n",
    "coef = log.coef_.tolist()[0]\n",
    "most_valuable_features = [coef.index(i) for i in sorted(coef)[:20]]\n",
    "features = np.array(cntvec.get_feature_names())\n",
    "\n",
    "pp.pprint(features[most_valuable_features])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
